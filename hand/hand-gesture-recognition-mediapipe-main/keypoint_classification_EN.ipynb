{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "igMyGnjE9hEp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: 'conda-forge::tensorflow': Expected end or semicolon (after name and no valid version specifier)\n",
      "    conda-forge::tensorflow\n",
      "               ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (6.32.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (2.2.6)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.3.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tengi\\downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.20.0-cp312-cp312-win_amd64.whl (331.9 MB)\n",
      "Installing collected packages: tensorflow\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\Users\\\\tengi\\\\Downloads\\\\hand-gesture-recognition-mediapipe-main\\\\hand-gesture-recognition-mediapipe-main\\\\.venv\\\\Lib\\\\site-packages\\\\tensorflow\\\\include\\\\external\\\\com_github_grpc_grpc\\\\src\\\\core\\\\ext\\\\filters\\\\gcp_authentication\\\\gcp_authentication_service_config_parser.h'\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall tensorflow\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m     10\u001b[39m RANDOM_SEED = \u001b[32m42\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tengi\\Downloads\\hand-gesture-recognition-mediapipe-main\\hand-gesture-recognition-mediapipe-main\\.venv\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "%pip install scikit-learn\n",
    "%pip install conda-forge::tensorflow\n",
    "%pip install tensorflow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2HDvhIu9hEr"
   },
   "source": [
    "# Specify each path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NvZP2Zn9hEy"
   },
   "outputs": [],
   "source": [
    "dataset = 'model/keypoint_classifier/keypoint.csv'\n",
    "model_save_path = 'model/keypoint_classifier/keypoint_classifier.hdf5'\n",
    "tflite_save_path = 'model/keypoint_classifier/keypoint_classifier.tflite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5oMH7x19hEz"
   },
   "source": [
    "# Set number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du4kodXL9hEz"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjnL0uso9hEz"
   },
   "source": [
    "# Dataset reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QT5ZqtEz9hE0"
   },
   "outputs": [],
   "source": [
    "X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmoKFsp49hE0"
   },
   "outputs": [],
   "source": [
    "y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQU7JTZ_9hE0"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, train_size=0.75, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxK_lETT9hE0"
   },
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHBmUf1t9hE1"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input((21 * 2, )),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(10, activation='relu'),\n",
    "    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypqky9tc9hE1",
    "outputId": "5db082bb-30e3-4110-bf63-a1ee777ecd46"
   },
   "outputs": [],
   "source": [
    "model.summary()  # tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MbMjOflQ9hE1"
   },
   "outputs": [],
   "source": [
    "# Model checkpoint callback\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    model_save_path, verbose=1, save_weights_only=False)\n",
    "# Callback for early stopping\n",
    "es_callback = tf.keras.callbacks.EarlyStopping(patience=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3Dac0M_9hE2"
   },
   "outputs": [],
   "source": [
    "# Model compilation\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7XI0j1Iu9hE2"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WirBl-JE9hE3",
    "outputId": "71b30ca2-8294-4d9d-8aa2-800d90d399de",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[cp_callback, es_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pxvb2Y299hE3",
    "outputId": "59eb3185-2e37-4b9e-bc9d-ab1b8ac29b7f"
   },
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "val_loss, val_acc = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBkmDeUW9hE4"
   },
   "outputs": [],
   "source": [
    "# Loading the saved model\n",
    "model = tf.keras.models.load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFz9Tb0I9hE4",
    "outputId": "1c3b3528-54ae-4ee2-ab04-77429211cbef"
   },
   "outputs": [],
   "source": [
    "# Inference test\n",
    "predict_result = model.predict(np.array([X_test[0]]))\n",
    "print(np.squeeze(predict_result))\n",
    "print(np.argmax(np.squeeze(predict_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3U4yNWx9hE4"
   },
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "id": "AP1V6SCk9hE5",
    "outputId": "08e41a80-7a4a-4619-8125-ecc371368d19"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, report=True):\n",
    "    labels = sorted(list(set(y_true)))\n",
    "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    \n",
    "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
    " \n",
    "    fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
    "    ax.set_ylim(len(set(y_true)), 0)\n",
    "    plt.show()\n",
    "    \n",
    "    if report:\n",
    "        print('Classification Report')\n",
    "        print(classification_report(y_test, y_pred))\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNP6aqzc9hE5"
   },
   "source": [
    "# Convert to model for Tensorflow-Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODjnYyld9hE6"
   },
   "outputs": [],
   "source": [
    "# Save as a model dedicated to inference\n",
    "model.save(model_save_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRfuK8Y59hE6",
    "outputId": "a4ca585c-b5d5-4244-8291-8674063209bb"
   },
   "outputs": [],
   "source": [
    "# Transform model (quantization)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantized_model = converter.convert()\n",
    "\n",
    "open(tflite_save_path, 'wb').write(tflite_quantized_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHBPBXdx9hE6"
   },
   "source": [
    "# Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGAzLocO9hE7"
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQuDK8YS9hE7"
   },
   "outputs": [],
   "source": [
    "# Get I / O tensor\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_ixAf_l9hE7"
   },
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], np.array([X_test[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s4FoAnuc9hE7",
    "outputId": "91f18257-8d8b-4ef3-c558-e9b5f94fabbf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Inference implementation\n",
    "interpreter.invoke()\n",
    "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vONjp19J9hE8",
    "outputId": "77205e24-fd00-42c4-f7b6-e06e527c2cba"
   },
   "outputs": [],
   "source": [
    "print(np.squeeze(tflite_results))\n",
    "print(np.argmax(np.squeeze(tflite_results)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "keypoint_classification_EN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
